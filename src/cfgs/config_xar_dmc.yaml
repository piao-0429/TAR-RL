# Copyright (c) Microsoft Corporation.
# Licensed under the MIT License.

# modified from DrQv2 config file

defaults:
  - _self_
  - task@_global_: walker_walk_rescale
  - override hydra/launcher: submitit_local

# task settings
debug: 1
# frame_stack: in paper actually used 3 as default for adroit, however
# it can take a lot more memory especially for relocate, and later ablation shows
# frame_stack does not affect performance significantly in Adroit, so here we set it to 1.
frame_stack: 3
action_repeat: 2
discount: 0.99
# eval
eval_every_frames: 50000
num_eval_episodes: 25
stage1_eval_every_frames: 50
stage2_eval_every_frames: 5000
stage2_num_eval_episodes: 5
# snapshot
save_snapshot: false
# replay buffer
replay_buffer_size: 1000000
replay_buffer_num_workers: 4
nstep: 3
batch_size: 256
# misc
seed: 1
device: cuda
save_video: false
save_train_video: false
use_wb: true
use_tb: false
save_models: false
local_data_dir: '/home/hp/TAR-RL/data'
show_computation_time_est: true # provide estimates on computation times
show_time_est_interval: 50
# experiment
project: TAR-RL-dmc
experiment: walker_walk_finetune_action_6
# environment
env_feature_type: 'pixels'
use_sensor: false
reward_rescale: true # TODO think about this...
# agent
lr: 1e-4
feature_dim: 50

# ====== stage 1 ======
visual_model_name: 'vrl3'
stage1_n_update: 5000
load_demo: true
num_demo: 25
# ====== stage 2 ======
stage2_n_update: 30000

# ====== stage 3 ======
num_seed_frames: 0

agent:
  _target_: xar_agent.XARAgent
  num_embodiments: ??? # to be specified later
  obs_shape: ??? # to be specified later
  action_shape: ??? # to be specified later
  action_idx: []  # to be specified later
  device: ${device}
  critic_target_tau: 0.01
  update_every_steps: 2
  use_wb: ${use_wb}
  lr: 1e-4
  hidden_dim: 1024
  feature_dim: 50
  act_rep_dim: 6
  policy_output_type: 'latent'
  deterministic_encoder: false
  deterministic_decoder: true

  data_dir: ${local_data_dir}

  # environment related
  use_sensor: ${use_sensor}

  # ====== stage 1 ======
  visual_model_name: ${visual_model_name}
  inv_weight: 1.0
  fwd_weight: 1.0
  kl_weight: 1.0

  # ====== stage 2, 3 ======
  use_data_aug: true
  encoder_lr_scale: 1
  stddev_clip: 0.3
  # safe Q
  safe_q_target_factor: 0.5 # value 1 is not using safe factor, value 0 is hard cutoff.
  safe_q_threshold: 200
  # pretanh penalty
  pretanh_threshold: 5
  pretanh_penalty: 0.001

  # ====== stage 2 ======
  stage2_update_visual_encoder: true # decides whether encoder is frozen or finetune in stage 2
  stage2_update_encoder: true
  stage2_update_decoder: true
  cql_weight: 1
  cql_temp: 1
  cql_n_random: 10
  stage2_std: 0.1
  stage2_bc_weight: 0.1 # ablation shows additional BC does not help performance

  # ====== stage 3 ======
  stage3_update_visual_encoder: true
  stage3_update_encoder: true
  stage3_update_decoder: true
  num_expl_steps: 0 # number of random actions at start of stage 3
  # std decay
  std0: 0.01
  std1: 0.01
  std_n_decay: 500000
  # bc decay
  stage3_bc_lam0: 0.01 # ablation shows additional BC does not help performance
  stage3_bc_lam1: 0.95

hydra:
  run: # this "dir" decides where the training logs are stored
    dir: ./data/logs/exp_local/${now:%Y.%m.%d}/${now:%H%M%S}_${hydra.job.override_dirname}
  sweep:
    dir: ./exp/${now:%Y.%m.%d}/${now:%H%M}_${agent_cfg.experiment}
    subdir: ${hydra.job.num}
  launcher:
    timeout_min: 4300
    cpus_per_task: 10
    gpus_per_node: 1
    tasks_per_node: 1
    mem_gb: 160
    nodes: 1
    submitit_folder: ./exp/${now:%Y.%m.%d}/${now:%H%M%S}_${agent_cfg.experiment}/.slurm
